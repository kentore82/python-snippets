{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T08:23:05.313848Z",
     "start_time": "2018-02-15T08:23:05.292487Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f7b9c38add0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bindings and imports to get a Spark Session in Jupyter/Python\n",
    "\n",
    "bindings=\"/share/hadoop_custom/conf/spark/spark_2.2.0_bindings.py\"\n",
    "exec(open(bindings).read())\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sconf = SparkConf()\n",
    "\n",
    "sconf.set('spark.submit.deployMode','client')\n",
    "sconf.set('spark.master','local')\n",
    "\n",
    "\n",
    "sconf.set('spark.shuffle.service.enabled',True)\n",
    "sconf.set('spark.dynamicAllocation.enabled',False)\n",
    "sconf.set('spark.executor.memory','1G')\n",
    "sconf.set('spark.driver.memory','1G')\n",
    "sconf.set('spark.executor.instances','4')#Number of executors\n",
    "sconf.set('spark.executor.cores','1') # number of cores on same worker\n",
    "\n",
    "app_name=\"NetCDFSpark\"\n",
    "sconf.set('spark.app.name',app_name) \n",
    "\n",
    "spark = ( SparkSession\n",
    "    .builder\n",
    "    .config(conf=sconf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:23:46.858616Z",
     "start_time": "2018-02-15T09:23:46.853453Z"
    }
   },
   "outputs": [],
   "source": [
    "## Other imports\n",
    "from snakebite.client import Client # snakebite is a python api for hdfs \n",
    "                                    # needs to be swapped with a google bucket equivqlent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:23:46.858616Z",
     "start_time": "2018-02-15T09:23:46.853453Z"
    }
   },
   "outputs": [],
   "source": [
    "# What does raw data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:36:23.129602Z",
     "start_time": "2018-02-15T09:36:23.038056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,hei,200.0\n",
      "2,ho,400.9\n",
      "3,ha,43.23\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /mnt/hdfs/tmp/1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:42:18.604689Z",
     "start_time": "2018-02-15T09:42:18.398132Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/1.csv', '/tmp/2.csv', '/tmp/3.csv']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of file paths to process\n",
    "myNetCdfFilePahts=[\"/tmp/1.csv\",\"/tmp/2.csv\",\"/tmp/3.csv\"]\n",
    "\n",
    "# Create Spark RDD that reads data\n",
    "numPartitions = 3 # This number is key, because it controls Spark input parallelizm. \n",
    "                  # One partition can process one or more files provided they fit the size of the Spark container\n",
    "                  # \n",
    "                  # The size of the conatiner is typically controlled by configs such as:\n",
    "                  # 'spark.executor.memory', 'spark.executor.cores'\n",
    "\n",
    "inPathRDD = spark.sparkContext.parallelize(myNetCdfFilePahts,numPartitions)\n",
    "inPathRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:42:34.281199Z",
     "start_time": "2018-02-15T09:42:33.646186Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/tmp/1.csv',\n",
       "  [{'index': '1', 'name': 'hei', 'value': '200.0'},\n",
       "   {'index': '2', 'name': 'ho', 'value': '400.9'},\n",
       "   {'index': '3', 'name': 'ha', 'value': '43.23'}]),\n",
       " ('/tmp/2.csv',\n",
       "  [{'index': '1', 'name': 'hei', 'value': '200.0'},\n",
       "   {'index': '2', 'name': 'ho', 'value': '400.9'},\n",
       "   {'index': '3', 'name': 'ha', 'value': '43.23'}]),\n",
       " ('/tmp/3.csv',\n",
       "  [{'index': '1', 'name': 'hei', 'value': '200.0'},\n",
       "   {'index': '2', 'name': 'ho', 'value': '400.9'},\n",
       "   {'index': '3', 'name': 'ha', 'value': '43.23'}])]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map over each file path and apply the function myDataReader() to each fpath\n",
    "# In my example this function will return a tuple: (fpath, list[dict/json with the imported data])\n",
    "inPathDataRDD = inPathRDD.map(lambda fpath: myDataReader(fpath))\n",
    "inPathDataRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:42:54.793863Z",
     "start_time": "2018-02-15T09:42:54.565680Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'index': '1', 'name': 'hei', 'value': '200.0'},\n",
       "  {'index': '2', 'name': 'ho', 'value': '400.9'},\n",
       "  {'index': '3', 'name': 'ha', 'value': '43.23'}],\n",
       " [{'index': '1', 'name': 'hei', 'value': '200.0'},\n",
       "  {'index': '2', 'name': 'ho', 'value': '400.9'},\n",
       "  {'index': '3', 'name': 'ha', 'value': '43.23'}],\n",
       " [{'index': '1', 'name': 'hei', 'value': '200.0'},\n",
       "  {'index': '2', 'name': 'ho', 'value': '400.9'},\n",
       "  {'index': '3', 'name': 'ha', 'value': '43.23'}]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We dont care about the link between data and filepaths any more\n",
    "inDataRDD = inPathDataRDD.map(lambda mytuple: mytuple[1])\n",
    "inDataRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:43:08.461708Z",
     "start_time": "2018-02-15T09:43:07.773350Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "|index|name|value|\n",
      "+-----+----+-----+\n",
      "|    1| hei|200.0|\n",
      "|    2|  ho|400.9|\n",
      "|    3|  ha|43.23|\n",
      "|    1| hei|200.0|\n",
      "|    2|  ho|400.9|\n",
      "|    3|  ha|43.23|\n",
      "|    1| hei|200.0|\n",
      "|    2|  ho|400.9|\n",
      "|    3|  ha|43.23|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FlatMap and convert to DataFram for easy write\n",
    "outDF = inDataRDD.flatMap(lambda x: x).toDF()\n",
    "\n",
    "outDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:31:30.808934Z",
     "start_time": "2018-02-15T09:31:30.788610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write DF as Parquet/csv/or whatever\n",
    "outDF.write(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T09:44:00.900870Z",
     "start_time": "2018-02-15T09:44:00.885018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cusom data reader function. Swap this with NetCDF version\n",
    "def myDataReader(fpath):\n",
    "    # I use the snakebite client to import hdfs csv files as text\n",
    "    HDFSclient=Client('1.sherpa.client.sysedata.no',8020,use_trash=False)\n",
    "    dataGenerator=HDFSclient.text([\"/tmp/1.csv\"])\n",
    "    \n",
    "    dataList = []\n",
    "    \n",
    "    # Not very elegant with a double for loop here ;)\n",
    "    for item in dataGenerator:\n",
    "        splitNewline=item.strip().split('\\n')\n",
    "        for string in splitNewline:\n",
    "             splitList=string.split(',')\n",
    "             dataList.append({'index': splitList[0], 'name': splitList[1], 'value': splitList[2]})\n",
    "    \n",
    "    \n",
    "    return (fpath,dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-07T14:41:46.626556Z",
     "start_time": "2017-11-07T14:41:45.522880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
